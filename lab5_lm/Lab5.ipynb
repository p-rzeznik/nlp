{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. Read the documentation of Language modelling in the Transformers library.\n",
    "2. Download three Polish models from the Huggingface repository."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dc0d9a12db67e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:08:41.548020Z",
     "start_time": "2023-11-10T18:08:39.649863Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask_model_1 = pipeline(task=\"fill-mask\", model='allegro/herbert-base-cased')\n",
    "# fill_mask_model_2 = pipeline(task=\"fill-mask\", model='allegro/herbert-base-cased')\n",
    "# fill_mask_model_3 = pipeline(task=\"fill-mask\", model='allegro/herbert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def parse_preds(preds):\n",
    "    return [\n",
    "        {\n",
    "            \"score\": round(pred[\"score\"], 3),\n",
    "            \"token\": pred[\"token\"],\n",
    "            \"token_str\": pred[\"token_str\"],\n",
    "            \"sequence\": pred[\"sequence\"],\n",
    "        }\n",
    "        for pred in preds\n",
    "    ]\n",
    "def parse_pred(pred, name):\n",
    "    return {\n",
    "                \"model\": name,\n",
    "                \"score\": round(pred[\"score\"], 3),\n",
    "                \"token\": pred[\"token\"],\n",
    "                \"token_str\": pred[\"token_str\"],\n",
    "                \"sequence\": pred[\"sequence\"],\n",
    "            }\n",
    "\n",
    "def preds_to_df(parsed_preds_list):\n",
    "    return pd.DataFrame(*parsed_preds_list, index='model')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:45:15.485460200Z",
     "start_time": "2023-11-10T18:45:15.454194400Z"
    }
   },
   "id": "549a1e207564d2e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Devise a method to test if the langage model understands Polish cases. E.g. testing for nominal case could be expressed as \"Warszawa to największe [MASK]\", and the masked word should be in nominative case. Create sentences for each case."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e2a4d8ea98052ee"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.__init__() got multiple values for argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[103], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m preds \u001B[38;5;241m=\u001B[39m fill_mask_model_1(text, top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      4\u001B[0m k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mpreds_to_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mparse_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbert\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbert2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbert3\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[102], line 21\u001B[0m, in \u001B[0;36mpreds_to_df\u001B[1;34m(parsed_preds_list)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreds_to_df\u001B[39m(parsed_preds_list):\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparsed_preds_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: DataFrame.__init__() got multiple values for argument 'index'"
     ]
    }
   ],
   "source": [
    "text =  \"Warszawa to największe <mask>\"\n",
    "top_k = 1\n",
    "preds = fill_mask_model_1(text, top_k=1)\n",
    "k=0\n",
    "preds_to_df([parse_pred(preds[k], 'bert'), parse_pred(preds[k], 'bert2'), parse_pred(preds[k], 'bert3')])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:45:16.645262100Z",
     "start_time": "2023-11-10T18:45:16.537339Z"
    }
   },
   "id": "3adec7b86eeb4ce7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Devise a method to test long-range relationships such as gender. E.e. you can use two verbs where withe masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5f5d944c955abf7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.0994,\n  'token': 22636,\n  'token_str': 'wracam',\n  'sequence': 'Byłam w sklepie, kupiłam chleb i wracam do domu'},\n {'score': 0.043,\n  'token': 19464,\n  'token_str': 'poszła',\n  'sequence': 'Byłam w sklepie, kupiłam chleb i poszła do domu'},\n {'score': 0.0425,\n  'token': 35614,\n  'token_str': 'wróciłem',\n  'sequence': 'Byłam w sklepie, kupiłam chleb i wróciłem do domu'}]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  \"Byłam w sklepie, kupiłam chleb i <mask> do domu\"\n",
    "preds = fill_mask_model_1(text, top_k=3)\n",
    "\n",
    "parse_preds(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:08:59.748366400Z",
     "start_time": "2023-11-10T18:08:59.648900900Z"
    }
   },
   "id": "c63c4c380b530b62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Check if the model captures real-world knolwedge. For instance a sentence \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\" checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2365370271949a6e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.362,\n  'token': 17550,\n  'token_str': 'Woda',\n  'sequence': 'Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.'}]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  \"<mask> wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\"\n",
    "preds = fill_mask_model_1(text, top_k=1)\n",
    "\n",
    "parse_preds(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:11:05.729195100Z",
     "start_time": "2023-11-10T18:11:05.648282400Z"
    }
   },
   "id": "accd84251df65d1d"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.8841,\n  'token': 22591,\n  'token_str': 'górą',\n  'sequence': 'Najwyższą górą na świecie jest Mount Everest'}]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  \"Najwyższą <mask> na świecie jest Mount Everest\"\n",
    "preds = fill_mask_model_1(text, top_k=1)\n",
    "\n",
    "parse_preds(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:13:44.058140800Z",
     "start_time": "2023-11-10T18:13:43.995629600Z"
    }
   },
   "id": "b8241a5e6f6ba9"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.3849,\n  'token': 5185,\n  'token_str': 'większe',\n  'sequence': 'Na Księżycu przyśpieszenie grawitacyjne jest większe niż na Ziemii'},\n {'score': 0.2154,\n  'token': 8921,\n  'token_str': 'wyższe',\n  'sequence': 'Na Księżycu przyśpieszenie grawitacyjne jest wyższe niż na Ziemii'},\n {'score': 0.1163,\n  'token': 9662,\n  'token_str': 'mniejsze',\n  'sequence': 'Na Księżycu przyśpieszenie grawitacyjne jest mniejsze niż na Ziemii'}]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  \"Na Księżycu przyśpieszenie grawitacyjne jest <mask> niż na Ziemii\"\n",
    "preds = fill_mask_model_1(text, top_k=3)\n",
    "\n",
    "parse_preds(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:16:03.980050500Z",
     "start_time": "2023-11-10T18:16:03.884554300Z"
    }
   },
   "id": "780aa0dbdb29fc85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Take into accout the fact, that causal language models such as PapuGaPT2 or plT5, will only generate continuations of the sentenes, so the examples have to be created according to that paradigm.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b69dee3a3b2e5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "828dc2e717af9857"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
