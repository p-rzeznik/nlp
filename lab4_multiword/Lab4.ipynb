{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piotr Rzeźnik 402194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:49:05.742413200Z",
     "start_time": "2023-11-19T16:49:03.746699300Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:49:08.462156800Z",
     "start_time": "2023-11-19T16:49:05.726779Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"clarin-knext/fiqa-pl\", 'corpus')\n",
    "pd_dataset = dataset['corpus'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use SpaCy tokenizer API to tokenize the text from the law corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:49:15.832243100Z",
     "start_time": "2023-11-19T16:49:08.466673300Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.pl import Polish\n",
    "nlp = Polish()\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:49:15.847776400Z",
     "start_time": "2023-11-19T16:49:15.832243100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute bigram counts of downcased tokens. Given the sentence: \"The quick brown fox jumps over the lazy dog.\", the bigram counts are as follows:\n",
    "\"the quick\": 1\n",
    "\"quick brown\": 1\n",
    "\"brown fox\": 1\n",
    "...\n",
    "\"dog .\": 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:49:33.012965100Z",
     "start_time": "2023-11-19T16:49:32.951126800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', '.')]\n"
     ]
    }
   ],
   "source": [
    "doc = tokenizer('The quick brown fox jumps over the lazy dog.')\n",
    "bigrams = [(x1.text,x2.text) for x1,x2 in zip(doc[:-1], doc[1:])]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:50:41.727946Z",
     "start_time": "2023-11-19T16:50:02.846439Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "bigram_dict = defaultdict(lambda: 0)\n",
    "texts = pd_dataset['text'].apply(lambda s: s.lower())\n",
    "for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "    bigrams = [(x1.text,x2.text) for x1,x2 in zip(doc[:-1], doc[1:])]\n",
    "    for bigram in bigrams:\n",
    "        bigram_dict[bigram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:52:32.769973400Z",
     "start_time": "2023-11-19T16:52:29.018576700Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = \"\"\"[0-9,\\.\\?\\)\\(\"'\\!\\\\/-]\"\"\"\n",
    "filtered_bigram_dict = defaultdict(lambda : 0)\n",
    "for (s0, s1), count in bigram_dict.items():\n",
    "    if re.search(pattern, str(s0) + str(s1)) is None:\n",
    "        filtered_bigram_dict[(s0, s1)] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:52:46.673747400Z",
     "start_time": "2023-11-19T16:52:32.769973400Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_dict = defaultdict(lambda: 0)\n",
    "texts = pd_dataset['text'].apply(lambda s: s.lower())\n",
    "for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "    for token in doc:\n",
    "        unigram_dict[token.text] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use pointwise mutual information to compute the measure for all pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:52:50.078739400Z",
     "start_time": "2023-11-19T16:52:46.705444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb43f2f222b457c975771ad27c43730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1659331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "uni_sum = float(sum(unigram_dict.values()))\n",
    "bi_sum = float(sum(filtered_bigram_dict.values()))\n",
    "\n",
    "def pmi(word1, word2, unigram_freq, bigram_freq):\n",
    "    prob_word1 = unigram_freq[word1] / uni_sum\n",
    "    prob_word2 = unigram_freq[word2] / uni_sum\n",
    "    prob_word1_word2 = bigram_freq[(word1, word2)] / bi_sum\n",
    "    return math.log(prob_word1_word2/float(prob_word1*prob_word2),2) \n",
    "  \n",
    "pmi_dict = defaultdict(lambda : None)\n",
    "for (s0, s1), count in tqdm(filtered_bigram_dict.items()):\n",
    "    pmi_dict[(s0, s1)] = pmi(s0,s1, unigram_dict, filtered_bigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:52:51.235720300Z",
     "start_time": "2023-11-19T16:52:50.314624Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pmi_df = pd.DataFrame(pmi_dict.items(), columns=['bigram', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sort the word pairs according to that measure in the descending order and determine top 10 entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:52:51.884938800Z",
     "start_time": "2023-11-19T16:52:51.235720300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         bigram      count\n",
      "189043      (stylistkę, marikę)  23.520279\n",
      "1159280           (stora, enso)  23.520279\n",
      "607752         (pedrama, mirza)  23.520279\n",
      "763356   (cristinie, fernández)  23.520279\n",
      "1616285   (soltar, estupideces)  23.520279\n",
      "201040           (pozik, daude)  23.520279\n",
      "1279434  (calitate, superioară)  23.520279\n",
      "572711            (boko, haram)  23.520279\n",
      "201037      (betetzen, dituzte)  23.520279\n",
      "1365180  (verdadero, estiubalo)  23.520279\n"
     ]
    }
   ],
   "source": [
    "print(pmi_df.sort_values(by='count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:53:09.572993200Z",
     "start_time": "2023-11-19T16:53:09.135278300Z"
    }
   },
   "outputs": [],
   "source": [
    "even_more_filtered_bigram_dict = defaultdict(lambda : 0)\n",
    "for (s0, s1), count in filtered_bigram_dict.items():\n",
    "    if count >= 5:\n",
    "        even_more_filtered_bigram_dict[(s0, s1)] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:53:15.345205300Z",
     "start_time": "2023-11-19T16:53:15.057584100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec93be2bd4b343f49a1039fd099fbbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uni_sum = float(sum(unigram_dict.values()))\n",
    "bi_sum = float(sum(even_more_filtered_bigram_dict.values()))\n",
    "\n",
    "def pmi(word1, word2, unigram_freq, bigram_freq):\n",
    "    prob_word1 = unigram_freq[word1] / uni_sum\n",
    "    prob_word2 = unigram_freq[word2] / uni_sum\n",
    "    prob_word1_word2 = bigram_freq[(word1, word2)] / bi_sum\n",
    "    return math.log(prob_word1_word2/float(prob_word1*prob_word2),2) \n",
    "  \n",
    "pmi_dict = defaultdict(lambda : None)\n",
    "for (s0, s1), count in tqdm(even_more_filtered_bigram_dict.items()):\n",
    "    pmi_dict[(s0, s1)] = pmi(s0,s1, unigram_dict, even_more_filtered_bigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:53:15.422402900Z",
     "start_time": "2023-11-19T16:53:15.345205300Z"
    }
   },
   "outputs": [],
   "source": [
    "pmi_df_2 = pd.DataFrame(pmi_dict.items(), columns=['bigram', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:53:15.453689600Z",
     "start_time": "2023-11-19T16:53:15.422402900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     bigram      count\n",
      "121548       (мою, команду)  21.876568\n",
      "119164     (królicza, nora)  21.876568\n",
      "58624   (klęska, żywiołowa)  21.876568\n",
      "106236    (bert, hellinger)  21.876568\n",
      "121549        (моя, группа)  21.876568\n",
      "121544       (олимп, трейд)  21.876568\n",
      "121543     (опционы, олимп)  21.876568\n",
      "121542   (инарные, опционы)  21.876568\n",
      "135594  (остались, вопросы)  21.876568\n",
      "133418  (stucco, veneziano)  21.876568\n"
     ]
    }
   ],
   "source": [
    "print(pmi_df_2.sort_values(by='count', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use KRNNT or Clarin-PL API to tag and lemmatize the corpus. Note: Clarin allows to upload a ZIP file with the whole corpus and process it as one request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:53:18.758851600Z",
     "start_time": "2023-11-19T16:53:18.308853900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nie',\n",
       " 'mówić',\n",
       " ',',\n",
       " 'że',\n",
       " 'nie',\n",
       " 'podobać',\n",
       " 'ja',\n",
       " 'się',\n",
       " 'też',\n",
       " 'pomysł',\n",
       " 'szkolenie',\n",
       " 'w',\n",
       " 'miejsce',\n",
       " 'praca',\n",
       " ',',\n",
       " 'ale',\n",
       " 'nie',\n",
       " 'móc',\n",
       " 'oczekiwać',\n",
       " ',',\n",
       " 'że',\n",
       " 'firma',\n",
       " 'to',\n",
       " 'zrobić',\n",
       " '.',\n",
       " 'szkolenie',\n",
       " 'pracownik',\n",
       " 'to',\n",
       " 'nie',\n",
       " 'on',\n",
       " 'praca',\n",
       " '–',\n",
       " 'on',\n",
       " 'tworzyć',\n",
       " 'oprogramowanie',\n",
       " '.',\n",
       " 'być',\n",
       " 'móc',\n",
       " 'system',\n",
       " 'edukacyjny',\n",
       " 'w',\n",
       " 'Stany',\n",
       " 'Zjednoczony',\n",
       " '(',\n",
       " 'lub',\n",
       " 'on',\n",
       " 'student',\n",
       " ')',\n",
       " 'powinien',\n",
       " 'trochę',\n",
       " 'martwić',\n",
       " 'się',\n",
       " 'o',\n",
       " 'zdobyć',\n",
       " 'umiejętność',\n",
       " 'rynkowy',\n",
       " 'w',\n",
       " 'zamian',\n",
       " 'za',\n",
       " 'on',\n",
       " 'ogromny',\n",
       " 'inwestycja',\n",
       " 'w',\n",
       " 'edukacja',\n",
       " ',',\n",
       " 'zamiast',\n",
       " 'wychodzić',\n",
       " 'z',\n",
       " 'tysiąc',\n",
       " 'zadłużyć',\n",
       " 'student',\n",
       " 'i',\n",
       " 'narzekać',\n",
       " ',',\n",
       " 'że',\n",
       " 'nie',\n",
       " 'być',\n",
       " 'do',\n",
       " 'nic',\n",
       " 'wykwalifikować',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post('http://localhost:9200',data=pd_dataset.iloc[0]['text'].encode('utf-8'))\n",
    "res.text.split()[2::5] # 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Using the tagged corpus compute bigram statistic for the tokens containing: a. lemmatized, downcased word b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:59:10.630983100Z",
     "start_time": "2023-11-19T16:59:10.615321200Z"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for x1,x2 in zip(res.text.split()[2::5], res.text.split()[3::5]):\n",
    "    result.append(f\"{x1}:{x2.split(':')[0]}\")\n",
    "# concat(\"1:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T16:59:13.630582900Z",
     "start_time": "2023-11-19T16:59:13.625886200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nie:qub',\n",
       " 'mówić:fin',\n",
       " ',:interp',\n",
       " 'że:comp',\n",
       " 'nie:qub',\n",
       " 'podobać:fin',\n",
       " 'ja:ppron12',\n",
       " 'się:qub',\n",
       " 'też:qub',\n",
       " 'pomysł:subst',\n",
       " 'szkolenie:subst',\n",
       " 'w:prep',\n",
       " 'miejsce:subst',\n",
       " 'praca:subst',\n",
       " ',:interp',\n",
       " 'ale:conj',\n",
       " 'nie:qub',\n",
       " 'móc:fin',\n",
       " 'oczekiwać:inf',\n",
       " ',:interp',\n",
       " 'że:comp',\n",
       " 'firma:subst',\n",
       " 'to:subst',\n",
       " 'zrobić:fin',\n",
       " '.:interp',\n",
       " 'szkolenie:subst',\n",
       " 'pracownik:subst',\n",
       " 'to:pred',\n",
       " 'nie:qub',\n",
       " 'on:ppron3',\n",
       " 'praca:subst',\n",
       " '–:interp',\n",
       " 'on:ppron3',\n",
       " 'tworzyć:fin',\n",
       " 'oprogramowanie:subst',\n",
       " '.:interp',\n",
       " 'być:inf',\n",
       " 'móc:fin',\n",
       " 'system:subst',\n",
       " 'edukacyjny:adj',\n",
       " 'w:prep',\n",
       " 'Stany:subst',\n",
       " 'Zjednoczony:adj',\n",
       " '(:interp',\n",
       " 'lub:conj',\n",
       " 'on:ppron3',\n",
       " 'student:subst',\n",
       " '):interp',\n",
       " 'powinien:winien',\n",
       " 'trochę:adv',\n",
       " 'martwić:inf',\n",
       " 'się:qub',\n",
       " 'o:prep',\n",
       " 'zdobyć:ger',\n",
       " 'umiejętność:subst',\n",
       " 'rynkowy:adj',\n",
       " 'w:prep',\n",
       " 'zamian:burk',\n",
       " 'za:prep',\n",
       " 'on:ppron3',\n",
       " 'ogromny:adj',\n",
       " 'inwestycja:subst',\n",
       " 'w:prep',\n",
       " 'edukacja:subst',\n",
       " ',:interp',\n",
       " 'zamiast:conj',\n",
       " 'wychodzić:inf',\n",
       " 'z:prep',\n",
       " 'tysiąc:subst',\n",
       " 'zadłużyć:ppas',\n",
       " 'student:subst',\n",
       " 'i:conj',\n",
       " 'narzekać:inf',\n",
       " ',:interp',\n",
       " 'że:comp',\n",
       " 'nie:qub',\n",
       " 'być:fin',\n",
       " 'do:prep',\n",
       " 'nic:subst',\n",
       " 'wykwalifikować:ppas',\n",
       " '.:interp']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T17:44:51.008984900Z",
     "start_time": "2023-11-19T17:34:06.464517200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cebc1749d146d99d88391363a08fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_dict_lemma = defaultdict(lambda: 0)\n",
    "unigram_dict_lemma = defaultdict(lambda: 0)\n",
    "trigram_dict_lemma = defaultdict(lambda: 0)\n",
    "for batch in tqdm(range(0, len(pd_dataset)//20, 50)):\n",
    "    for text in pd_dataset.iloc[batch:batch+50]['text']:\n",
    "        res = requests.post('http://localhost:9200',data=text.encode('utf-8'))\n",
    "        doc = []\n",
    "        for x1,x2 in zip(res.text.split()[2::5], res.text.split()[3::5]):\n",
    "            doc.append(f\"{x1}:{x2.split(':')[0]}\")\n",
    "            unigram_dict_lemma[doc[-1]] += 1\n",
    "        bigrams = [(x1,x2) for x1,x2 in zip(doc[:-1], doc[1:])]\n",
    "        for bigram in bigrams:\n",
    "            bigram_dict_lemma[bigram] += 1\n",
    "        trigrams = [(x1,x2,x3) for x1,x2,x3 in zip(doc[:-2], doc[1:-1], doc[2:])]\n",
    "        for trigram in trigrams:\n",
    "            trigram_dict_lemma[trigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T17:46:24.287662Z",
     "start_time": "2023-11-19T17:46:22.104007500Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_dict = defaultdict(lambda: 0)\n",
    "unigram_dict = defaultdict(lambda: 0)\n",
    "trigram_dict = defaultdict(lambda: 0)\n",
    "texts = pd_dataset.iloc[0:len(pd_dataset)//20]['text'].apply(lambda s: s.lower())\n",
    "for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "    for token in doc:\n",
    "        unigram_dict[token.text] += 1\n",
    "    bigrams = [(x1.text,x2.text) for x1,x2 in zip(doc[:-1], doc[1:])]\n",
    "    for bigram in bigrams:\n",
    "        bigram_dict[bigram] += 1\n",
    "    trigrams = [(x1.text,x2.text,x3.text) for x1,x2,x3 in zip(doc[:-2], doc[1:-1], doc[2:])]\n",
    "    for trigram in trigrams:\n",
    "        trigram_dict[trigram] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences.\n",
    "11. Compute trigram counts for both corpora and perform the same filtering.\n",
    "12. Use PMI (with 5 occurrence threshold) to compute top 10 results for the trigrams. Devise a method for computing the values, based on the results for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:11:39.632235700Z",
     "start_time": "2023-11-19T18:11:39.616611100Z"
    }
   },
   "outputs": [],
   "source": [
    "uni_sum = float(sum(unigram_dict.values()))\n",
    "bi_sum = float(sum(bigram_dict.values()))\n",
    "tri_sum = float(sum(trigram_dict.values()))\n",
    "\n",
    "uni_sum_lemma = float(sum(unigram_dict_lemma.values()))\n",
    "bi_sum_lemma = float(sum(bigram_dict_lemma.values()))\n",
    "tri_sum_lemma = float(sum(trigram_dict_lemma.values()))\n",
    "\n",
    "def bi_pmi(word1, word2, unigram_freq,bigram_freq, uni_sum, bi_sum_):\n",
    "    prob_word1 = unigram_freq[word1] / uni_sum\n",
    "    prob_word2 = unigram_freq[word2] / uni_sum\n",
    "    prob_word1_word2 = bigram_freq[(word1, word2)] / bi_sum_\n",
    "    return math.log(prob_word1_word2/float(prob_word1*prob_word2),2)\n",
    "\n",
    "def tri_pmi(word1, word2, word3, unigram_freq, trigram_freq, uni_sum, tri_sum_):\n",
    "    prob_word1 = unigram_freq[word1] / uni_sum\n",
    "    prob_word2 = unigram_freq[word2] / uni_sum\n",
    "    prob_word3 = unigram_freq[word3] / uni_sum\n",
    "    prob_word1_word2 = trigram_freq[(word1, word2, word3)] / tri_sum_\n",
    "    return math.log(prob_word1_word2/float(prob_word1*prob_word2*prob_word3),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:11:42.910466400Z",
     "start_time": "2023-11-19T18:11:42.480928600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eda1afd89044d895a842289a371218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_pmi_dict = defaultdict(lambda : None)\n",
    "for (s0, s1), count in tqdm(bigram_dict.items()):\n",
    "    bigram_pmi_dict[(s0, s1)] = bi_pmi(s0,s1, unigram_dict, bigram_dict, uni_sum, bi_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:11:43.559483200Z",
     "start_time": "2023-11-19T18:11:42.908247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7cccac7d07442eb074d08f9317d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_pmi_dict = defaultdict(lambda : None)\n",
    "for (s0, s1, s2), count in tqdm(trigram_dict.items()):\n",
    "    trigram_pmi_dict[(s0, s1, s2)] = tri_pmi(s0,s1,s2, unigram_dict, trigram_dict, uni_sum, tri_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lammatized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:11:43.933581100Z",
     "start_time": "2023-11-19T18:11:43.579726900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acd42a9847f44daa944ba8196a97d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_pmi_dict_lemma = defaultdict(lambda: None)\n",
    "for (s0, s1), count in tqdm(bigram_dict_lemma.items()):\n",
    "    bigram_pmi_dict_lemma[(s0, s1)] = bi_pmi(s0, s1, unigram_dict_lemma, bigram_dict_lemma, uni_sum_lemma, bi_sum_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:11:44.510038600Z",
     "start_time": "2023-11-19T18:11:43.933581100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2c1f3c32b445ce9880a2de0636274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_pmi_dict_lemma = defaultdict(lambda: None)\n",
    "for (s0, s1, s2), count in tqdm(trigram_dict_lemma.items()):\n",
    "    trigram_pmi_dict_lemma[(s0, s1, s2)] = tri_pmi(s0, s1, s2, unigram_dict_lemma, trigram_dict_lemma, uni_sum_lemma, tri_sum_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to dataframes:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:14:02.740847200Z",
     "start_time": "2023-11-19T18:14:02.657848800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 bigram      count\n",
      "162416  (ogólnokrajowymi, gospodarzami)  18.610913\n",
      "167595            (głównemu, oficerowi)  18.610913\n",
      "105796            (imprezowym, szyldem)  18.610913\n",
      "105772        (cheerleaderki, milczały)  18.610913\n",
      "105771  (republikańskie, cheerleaderki)  18.610913\n",
      "62429       (wydzierżawiona, telekomom)  18.610913\n",
      "167770          (nakarmiłbym, jebanego)  18.610913\n",
      "32883              (uzdolnionej, córce)  18.610913\n",
      "167767              (obrzydliwy, szlam)  18.610913\n",
      "3513                  (końskiej, pyska)  18.610913\n"
     ]
    }
   ],
   "source": [
    "bigram_pmi_df = pd.DataFrame(bigram_pmi_dict.items(), columns=['bigram', 'count'])\n",
    "print(bigram_pmi_df.sort_values(by='count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:14:13.508403600Z",
     "start_time": "2023-11-19T18:14:13.405759800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   bigram      count\n",
      "126348                  (pomiary:space, obejmujące:space)  18.649206\n",
      "42462                     (pokazywać:ger, kwotowań:subst)  18.649206\n",
      "57054                          (myśliwy:subst, se:interj)  18.649206\n",
      "161531  (none:czerwony, space:dit.com/message/compose?...  18.649206\n",
      "169125                  (obniżysz:space, francuską:space)  18.649206\n",
      "57086   (https://www.reuters.com/article/us-usa-sec-co...  18.649206\n",
      "139368                         (Aarjav:subst, Skin:subst)  18.649206\n",
      "139369                           (Skin:subst, Care:subst)  18.649206\n",
      "169106             (podwójnego:none, opodatkowania:space)  18.649206\n",
      "57195                          (Mini:subst, Punjab:subst)  18.649206\n"
     ]
    }
   ],
   "source": [
    "bigram_pmi_df_lemma = pd.DataFrame(bigram_pmi_dict_lemma.items(), columns=['bigram', 'count'])\n",
    "print(bigram_pmi_df_lemma.sort_values(by='count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:14:59.384015400Z",
     "start_time": "2023-11-19T18:14:59.191567700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  trigram      count\n",
      "99765               (niewolników, nazywano, niewolnikami)  37.221903\n",
      "224115                         (^również, ^usunie, ^przy)  37.221903\n",
      "10979   (https://sciencemag.org, http://www.sciencemag...  37.221903\n",
      "184413                       (rozerwą, społeczną, tkankę)  37.221903\n",
      "24794                  (kalibracja, pomiarowa, us4782698)  37.221903\n",
      "34911                             (която, изтрива, данни)  37.221903\n",
      "224117                   (^przy, ^komentarzu, ^zdobędzie)  37.221903\n",
      "224116                      (^usunie, ^przy, ^komentarzu)  37.221903\n",
      "34904                                 (паметта, е, форма)  37.221903\n",
      "280073               (rozkładów, modelujących, wariancję)  37.221903\n"
     ]
    }
   ],
   "source": [
    "trigram_pmi_df = pd.DataFrame(trigram_pmi_dict.items(), columns=['trigram', 'count'])\n",
    "print(trigram_pmi_df.sort_values(by='count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:15:03.524222600Z",
     "start_time": "2023-11-19T18:15:03.406191100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  trigram      count\n",
      "215942  (disamb:przestrzeni, disamb:długich, disamb:ok...  37.298486\n",
      "167415           (wielką:space, dojną:space, krową:space)  37.298486\n",
      "38835        (newline:najechać, space:kursor, space:mysz)  37.298486\n",
      "69814   (space:wdrażać, space:przyspieszać, space:niew...  37.298486\n",
      "260748        (CPGgrey's:subst, Humans:subst, Need:subst)  37.298486\n",
      "69798        (space:głęboki, space:zaleta, space:łańcuch)  37.298486\n",
      "134667      (Warren:newline, Buffett:space, słynął:space)  37.298486\n",
      "264372    (narysować:impt, zniesienie:subst, fibba:subst)  37.298486\n",
      "225953        (analizę:space, due:space, diligence:space)  37.298486\n",
      "209308  (nacisnąć:inf, pedał:subst, przyspieszenie:subst)  37.298486\n"
     ]
    }
   ],
   "source": [
    "trigram_pmi_df_lemma = pd.DataFrame(trigram_pmi_dict_lemma.items(), columns=['trigram', 'count'])\n",
    "print(trigram_pmi_df_lemma.sort_values(by='count', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
